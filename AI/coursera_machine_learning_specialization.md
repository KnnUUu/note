# 1.1
æœºå™¨å­¦ä¹ çš„åº”ç”¨ï¼šå¹¿å‘Šæ¨é€ã€å›¾åƒè¯†åˆ«ã€åŒ»å­¦è¯Šæ–­è¾…åŠ©ã€‚ã€‚ã€‚  
# 1.2
# 2.1
ä½¿ç”¨é¢‘ç‡ï¼šç›‘ç£å­¦ä¹ ï¼ˆ90%ä»¥ä¸Šï¼‰ -> éç›‘ç£å­¦ä¹  -> å¼ºåŒ–å­¦ä¹   
# 2.2
å›å½’ regressionï¼šä»æ— æ•°å¯èƒ½æ•°å­—ä¸­é¢„æµ‹ä¸€ä¸ªæ•°å­—  
# 2.3
åˆ†ç±» classificationï¼šåœ¨æœ‰é™çš„ç»“æœé‡Œé¢„æµ‹å…¶ä¸­ä¸€ä¸ª  
# 2.4
unsupervised learning: find something interesting in unlabeled data  
clustering èšç±»: put unlabeled data into different clusters  
åº”ç”¨ï¼šgoogle newsã€äººç±»åŸºå› ç»„åˆ†ç±»ã€ç»™äºˆæ•°æ®å°†å®¢æˆ·åˆ†ç±»  
# 2.5
anomaly detection: find unusual data points  
è¾“å…¥ä¸€å †é‚®ä»¶æ•°æ®ï¼Œè®©ç®—æ³•è‡ªå·±åˆ†ç±»  
è¾“å…¥é‡‘èæ•°æ®ï¼Œæ£€æµ‹å¼‚å¸¸  
# 2.6
# 2.7
linear regression çº¿æ€§å›å½’ï¼šä¸ºæ•°æ®æ‹Ÿåˆä¸€æ¡ç›´çº¿  
trainnig set: ç”¨äºè®­ç»ƒæ¨¡å‹  
xï¼šè¾“å…¥  
yï¼šè¾“å‡º  
mï¼šè®­ç»ƒæ•°æ®é‡  
(x,y)ï¼šå•ä¸ªè®­ç»ƒæ•°æ®  
(x<sup>(i)</sup>,y<sup>(i)</sup>)ï¼šç¬¬iä¸ªæ•°æ®  
# 2.8
$\hat{y}$ï¼šyçš„ä¼°è®¡æˆ–é¢„æµ‹  
yï¼šè®­ç»ƒé›†ä¸­çš„çœŸå®å€¼  
fï¼šæ¨¡å‹  

|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |
|:------------|:------------------------------------------------------------|----|
| $a$ | scalar, non bold                                                      ||
| $\mathbf{a}$ | vector, bold                                                      ||
| **Regression** |         |    |     |
|  $\mathbf{x}$ | Training Example feature values | `x_train` |   
|  $\mathbf{y}$  | Training Example targets  | `y_train` 
|  $x^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `x_i`, `y_i`|
| m | Number of training examples | `m`|
|  $w$  |  parameter: weight,                                 | `w`    |
|  $b$           |  parameter: bias                                           | `b`    |     
| $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | `f_wb` | 

# 3.3
parameterå‚æ•°ï¼šä¹Ÿè¢«å«åš coefficientç³»æ•° æˆ– weightæƒé‡  
cost function æˆæœ¬å‡½æ•°ï¼šç”¨æ¥è¡¡é‡æ¨¡å‹æ‹Ÿåˆç¨‹åº¦çš„å‡½æ•°   
error è¯¯å·®ï¼šé¢„æµ‹å€¼ä¸å®é™…å€¼çš„å·®è·  

# 3.4
nï¼šå‚æ•°ç»´åº¦æ•°é‡  
 <p align="center">
  <img width="90%" src="images/multiple_features_1.png">
</p>
 <p align="center">
  <img width="90%" src="images/multiple_features_2.png">
</p>

# 3.5
1ç»´ï¼šç¬‘è„¸çŠ¶å‡½æ•°  
2ç»´ï¼šç¢—çŠ¶å‡½æ•°  

ç­‰é«˜å›¾ï¼šlossä¸€æ ·çš„ç‚¹ï¼Œæƒ³è±¡ä¸€ä¸‹å±±çš„ç­‰é«˜å›¾  

# 3.6
ğŸ‘†çš„å…·ä½“ä¾‹å­  

# 4.1 æ¢¯åº¦ä¸‹é™
squared error cost functionï¼ˆå¹³æ–¹è¯¯å·®ä»£ä»·å‡½æ•°ï¼‰ï¼šæŒ‡MSE  
å¯¹äºå…·æœ‰å¹³æ–¹è¯¯å·®ä»£ä»·å‡½æ•°çš„çº¿æ€§å›å½’ï¼Œå‡½æ•°æ€»æ˜¯å‘ˆå¼“å½¢æˆ–è€…åŠåºŠå½¢  
ä½†åœ¨å®é™…æƒ…å†µä¸­ï¼Œå¯ä»¥æœ‰å¤šä¸ªä½è°·ï¼Œå«åšå±€éƒ¨æå°å€¼ local minima  
<p align="center">
  <img width="90%" src="images/gradient_descent.png">
</p>

### 4.2 å®ç°æ¢¯åº¦ä¸‹é™
å‡è®¾losså‡½æ•°æ˜¯äºŒç»´çš„ï¼Œæœ‰ $w, b$ ä¸¤ä¸ªå‚æ•°  
åˆ™å‚æ•°ä»¥ä»¥ä¸‹çš„æ–¹å¼æ›´æ–°  

$$
\begin{aligned}
w^{(t+1)} &= w^{(t)} - \alpha\ \frac{\partial J(w^{(t)}, b^{(t)})}{\partial w},\\
b^{(t+1)} &= b^{(t)} - \alpha\ \frac{\partial J(w^{(t)}, b^{(t)})}{\partial b},
\end{aligned}
$$
$ Î± $ï¼šå­¦ä¹ é€Ÿç‡ï¼ˆlearning rateï¼‰  
$ t $ï¼šå‚æ•°æ›´æ–°æ¬¡æ•°  
$  J(w^{(t)}, b^{(t)}) $ï¼šæŸå¤±å‡½æ•°  

æ³¨æ„å„ä¸ªå‚æ•°å¿…é¡»**åŒæ—¶**æ›´æ–°ï¼Œä¸èƒ½ç°æ›´æ–°$ w $å†æ›´æ–°$ b $  

### 4.3 æ¢¯åº¦ä¸‹é™çš„ç›´è§‚ç†è§£
å¯¼æ•°æ˜¯æ–œç‡ï¼Œæ–œç‡è¶Šé«˜ç§»åŠ¨çš„é€Ÿåº¦è¶Šå¿«ï¼Œæ–œç‡ä½ç§»åŠ¨æ…¢  
åœ¨å‡½æ•°åº•éƒ¨å·¦è¾¹æ—¶ï¼Œæ–œç‡ä¸ºæ­£å‘å³ç§»åŠ¨ï¼Œåœ¨å³è¾¹æ—¶å‘å·¦ç§»åŠ¨  